---
typora-root-url: ..\..\static\
title: "Conv1d原理解析"
date: 2021-10-27T19:00:02+08:00
tags: ["学习笔记"]
categories: ["计算机课程学习笔记"]

summary: "torch中的Conv1d解析"
---

## CS234: Reinforcement Learning

Standford公开课，我看了一下我觉得还是很有必要学习一下，大概花费时间为两天左右。

### 1 Introduction

课程内容：

* Overview of RL
* Course logistics
* Introduction to sequence decision process under uncertainty

sequence of decisions

* Optimization
  * goal: make good decisions
  * decisions can impact things much later
  * challenge:
    * when planning
    * when learning 

* exploration
  * reward作为label
* policy: map from past experience to action
* generalization
  * 为什么不预设置一个动作空间：状态过多，而且（有些时候）可能发生改变

sequential decision making

* agent -> action -> world -> observation reward -> agent, a loop

* goal: select actions to maximize total expected future reward
  * may balance immediate and long-term reward
  * may require strategic behavior to achieve high reward


state  $s_t$ is Markov 当且仅当 $p(s_{t+1}|s_t,a_t) = p(s_{t+1}|h_t,a_t)$，$h_t$为history
* make world markov
* 因此state需要具备足够多的信息，让其重复，让其具备马尔科夫特性

policy $\pi$, determines how the agent choose actions
* $\pi$ map $S \to A$, mapping from states to actions
* deterministic policy, $\pi (s) = a$
* stochastic policy, $\pi (a|s) = Pr(a_t = a | s_t = s)$

Value function $V^{\pi}$: expected discounted sum of future rewards under a particular policy $\pi$
* discount factor $\gamma \in (0,1)$，权衡immediate reward vs future reward

* model-based:
  * explicit: model
  * transition model or reward model
* model-free
  * explicit value function and reward function
  * no model


planning: given model of how the world works
* compute act in order to maximize expected reward

exploration and exploitation

## 2 Given a model of the world

应该是关于model-based的

model: mathematical models of dynamic and reward